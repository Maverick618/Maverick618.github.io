# 数据集与数据处理
## [内置数据集](https://pytorch.org/vision/stable/datasets.html#built-in-datasets)
PyTorch为深度学习领域中经常使用的一些数据集提供了便携的接口，并针对不同的数据集进行了简单的分类，使得开发者能够方便快捷地使用数据集。本节标题[链接](https://pytorch.org/vision/stable/datasets.html#built-in-datasets)的是torchvision中内置的数据集，下面的示例也将使用torchvision中内置的CIFAR10数据集。
```python title='（1）使用CIFAR10数据集'
import torchvision

train_set = torchvision.datasets.CIFAR10(
    root="./dataset"，  # 数据集存放的目录，这里采用的是相对路径
    train=True,         # 需要的数据是否为训练数据集
    download=True)      # 如果数据集不存在是否进行联网下载数据集
test_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True)
```
## 创建数据集
尽管PyTorch提供了相当可观的内置数据集，但在一些情况下，我们可能需要使用自己已有的数据。这种情况下，PyTorch为我们提供了Dataset工具用于创建自己的数据集。
!!! note "建议"
    对于深度学习网络的初学者，建议使用内置数据集。使用Dataset创建自己的数据集可以以后再学。
### 文件夹分类数据集
我这里所说的文件夹分类数据集是指在分类工作中，同一类的数据都放在同一个文件夹下，同时这些数据所属的类别就是文件夹的名字。简单来说，这种数据集可以表示为：
```
- train
    - tag_01
        - data_01_01
        - data_01_02
        ···
    - tag_02
        - data_02_01
        - data_02_02
        ···
- test
    - tag_01
        - data_01_01
        - data_01_02
        ···
    - tag_02
        - data_02_01
        - data_02_02
        ···
```
如此，我们可以使用Dataset工具来构建这种简单的数据集。
```python title="（2）dataset示例01"
from torch.utils.data import Dataset
from PIL import Image
import os

class MyData(Dataset): # 一般而言，继承Dataset类并重写两个方法即可构建自己的数据集啦
    def __init__(self, root_dir, label):
        # 储存路径信息
        self.root_dir = root_dir
        self.label = label
        self.data_path = os.path.join(root_dir, label)
        # 储存数据信息
        self.img_path = os.listdir(self.data_path)
    
    def __getitem__(self, idx):
        # 获取目标路径
        item_path = self.img_path[idx]
        item_path = os.path.join(self.data_path, item_path)
        # 打开文件
        img = Image.open(img_path)
        label = self.label
        return img, label
```
### 标签文件数据集
我所说的标签文件数据集是指这种数据集的每一个数据都有与之一一对应的*标签文件*。**简单**来说，这种数据集可以表示为：
```
    - data_01
        - data_01_01.xxx
        ···
    - lable_01
        - data_01_01.txt (也可能是其他格式)
```
这样的数据集构建起来和代码（2）也没多大差别。区别主要体现在对标签的处理上，（2）中标签是已知的，直接返回即可。而（3）中需要根据路径打开标签文件获取相应的标签。
```python title="（3）dataset示例02"
from torch.utils.data import Dataset
from PIL import Image
import os

class MyData(Dataset):
    def __init__(self, data_dir, label_dir):
        self.data_path = data_dir
        self.label_dir = label_dir
        self.img_path = os.listdir(self.data_path)
        self.label_path = os.listdir(self.data_path)
    
    def __getitem__(self, idx):
        item_path = self.img_path[idx]
        item_path = os.path.join(self.data_path, item_path)
        item_label_path = self.label_path[idx]
        item_label_path = os.path.join(self.labelpath, item_label_path)
        # 获取数据
        img = Image.open(img_path)
        # 根据 item_label_path 获取 label，不同任务label的类型不同
        # label = ···
        return img, label
```
## 数据处理
我们现在获取到了数据，比如是一些图片，要做分类任务，这些图片可能大小就不一样，格式可能也不一样，这就需要我们对数据进行一些处理，才能够更好的给神经网络作为输入。下面主要介绍torchvision中的几种transforms。
### ToTensor
输入：PIL Image or ndarray

输出：tensor

示例：
```python title="（4）Totensor"
# 先有变量img为PIL类型
from torchvision import transforms as T
totensor = T.ToTensor() # 定义一个转换器对象
img = totensor(img)     # 可调用对象
```
### Resize
>Resize the input image to the given size.

>If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions
### Normalize
>Normalize a tensor image with mean and standard deviation.

>This transform does not support PIL Image.

>Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n`` channels, this transform will normalize each channel of the input ``torch.*Tensor`` i.e.,

>``output[channel] = (input[channel] - mean[channel]) / std[channel]``
### Compose
Compose主要是用来整合一系列的操作，用于简化代码，Compose要求这一系列的处理中，前一个处理的输出能够作为下一个处理的输入。
```python title="（5）官方Compose示例"
    Example:
        transforms.Compose([
            transforms.CenterCrop(10),
            transforms.PILToTensor(),
            transforms.ConvertImageDtype(torch.float),
        ])
```
### MORE
更多的数据处理操作信息可以查看[PyTorch官方文档](https://pytorch.org/docs/stable/index.html)或直接查询源码。

## 数据加载
在PyTorch中我们一般使用[DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)来进行数据加载。DataLoader的主要作用是将数据从数据集中按一定规则加载进来，下面作简单的演示。
```python title="（6）DataLoader的简单使用"
    # 我们使用代码（1）中的数据集
    from torch.utils.data import DataLoader
    train_loader = DataLoader(
        dataset=train_set,   # 加载的数据集
        batch_size=128,      # 批量大小
        shuffle=True,        # 是否打乱数据
        num_works=0,         # 工作线程数
        drop_last=True       # 是否舍弃最后数量不足批量大小的那些数据
    ) # 更多参数参考官方文档或源码

    for epoch in range(10):
        for data in train_loader:
            # some operations for data
            # such as:
            # imgs, labels = data
            # ...
```

